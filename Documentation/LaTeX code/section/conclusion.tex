\section{Conclusion and Future Work}

\subsection{Conclusion}
A lot of work and hours were spent creating and optimizing all the processes.  These processes include from gathering data, to extracting details and display them in a user-friendly way.
Right now, a user can get data from the API automatically and concatenate it thanks to a script.
After that, all the user has to do is load the data into a Jupyter Notebook, which will automatically do the sentiment analysis.
The plot functions are ready, so the user will be able to plot all the data he imported.

This work has opened my eyes to the world of data. I had never considered sentiment analysis and having it in newspapers made me realize the importance of data in everyday life.

This work could very well be part of studying a psychological thesis on the masses, to see how the polarity of the news can affect the masses.
More specifically today where news is for the most part negative, see the influence it has on the individual person.

Personally, I find it very important how the news is given to us, whether it is true or not, and whether it is possible to improve this situation.

Perhaps through this work, people can realize the negativity of certain news outlets, becoming aware that the world is not as bad as they portray it.

\subsection{Future Work}
There is a lot of work that can be tackled in the future. Some ideas are listed in this section, but there is by far more potential.

\subsubsection{Back-End}
\subsubsection*{Improve data quality}
For the project I used two APIs to get data from the internet, for the future it would be better to do a manual scraping of the data.
By doing a manual scraping it is possible to choose many more newspapers, and especially take the ones that are more specific to the area.
Besides that, manual scraping, it allows to have much more data from a single newspaper.

\subsubsection*{Improve accuracy model}
The model has an accuracy of 93\%, this is because a specific dataset on movie reviews was used.
One way to increase the accuracy is to have different datasets, the best option would be to have a dataset on newspaper articles, so that the model already trains on different categories and specific topics, also learning the words better.

\subsubsection*{Improve technology}
Technology is always moving, for this reason it is better to keep updated the model or to create it with the technologies present at the moment of speaking.
In fact, just today has been released the new version of Tensorflow with many improvements.


\subsubsection{Front-End}
\subsubsection*{User interface}
The creation of an application with a user interface, would allow the user to be able to better use this data as a tool.

In addition, the user interface or website should better explain the statistics obtained, making it easier for the user.



\newpage
\section{Attachments}
In addition to the documentation, the following documents are available:
\begin{itemize}
    \item \textbf{"SentimentDE.html"}, the notebook with the source code of the model creation in html format,
    \item \textbf{"WorkOnNewspaper.html"}, the notebook with source of the sentiment analysis on newspaper in html format,
    \item \textbf{"Grid.ipynb"}, the notebook with source of le funzioni per il plot senza caricare il modello + leggero,
    \item \textbf{"SentimentOnSwissNewspaper.pdf"}, the source code of the project in pdf format
    \item \textbf{"Task.pdf"}, the schedule of the project tasks in pdf format
    \item \textbf{"BachelorTime.pdf"}, the schedule of the project in pdf format
    \item \textbf{"BachelorTime.xlsx"}, the schedule of the project in excel format
\end{itemize}

The code for the entire project, as well as the documentation and scripts, can be reached at the git repository of the bachelor thesis: \url{https://github.com/bak89/Sentiment-Analysis-on-Swiss-Newspapers}

