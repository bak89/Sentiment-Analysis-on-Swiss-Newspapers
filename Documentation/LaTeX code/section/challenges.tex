\section{Challenges}
In this section, I look at the challenges that emerged during the work on the project and how I covered them.
\subsection{Project scaling}
As written in the introduction, my partner in the team could not continue this project with me so under the advice of the professor, I decided to scale the project.
In doing so, the sentiment analysis went from 5 categories in the previous project to 2.

\subsection{Finding the appropriate datasets} 
My second problem was to find a suitable data set for my needs. To train a model, I needed a data set that was relevant for sentiment analysis, but also large enough to split into training and testing sets without losing model quality. In my opinion, on Kaggle and the internet in general everything can be found.
\subsection{Working environment}
\subsubsection{Virtual machine}
My third problem was on the virtual machine, the machine that was given to me unfortunately is not very fitting for the work that must be performed in terms of performance, in fact I have made a request for a more powerful machine. In the time in which I got the new machine, I moved to Colab, a service offered by google, in which a lot of dependencies are already installed.

\subsubsection{Google Colab}
The first problem I had with Colab is that every time we opened the notebook, I had to load the data, so this implied a big waste of time since the dataset is very large. I also tried to leave it in background on the virtual machine, but Colab after a certain time stops the runtime, so quite useless for my purpose, since to train the model takes several hours.
In the end, Anaconda turned out to be the appropriate solution.

\subsubsection{Anaconda and Jupyter Notebook}
After the difficulties in Colab I decided to go back to the virtual machine, this time more powerful. I created a requirements file from Colab, so I could install all the libraries needed in a new environment. I also tried to create an environment just for the project, and after installing all the dependencies I found errors in the notebook. After several tests on different env's I also had problems installing some libraries. I wanted to avoid working on the base env of anaconda, but it was the only solution that worked. Probably due to system administration privileges. But on the virtual machine I solved all the problems about imports, the dataset remained loaded and the \gls{Tensorflow} library was much faster.

\subsubsection{Tensorflow and Jupyter}
Unfortunately, even after troubleshooting Anaconda, I had several problems with \gls{Tensorflow} in Jupyter including:
\begin{itemize}
    \item ERROR: tensorflow-gpu 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.5 which is incompatible.
    I solved it with:
        \begin{itemize}
            \item \url{https://github.com/tensorflow/models/issues/9200}
            \item upgrade all the package with: !python -m pip install --upgrade pip
        \end{itemize}
    \item ERROR: Failed building wheel for pycocotools
    I solved it with:
        \begin{itemize}
            \item \url{https://github.com/cocodataset/cocoapi/issues/169}
            \item by installing Microsoft Visual C++ 14.0
        \end{itemize}
    \item ERROR: AttributeError: `float' object has no attribute `split'.
     The problem is that some reviews were null,I solved it with:
        \begin{itemize}
            \item test = test[test['Review'].notnull()]
            \item xtest\_list = x\_test.values.tolist()
            \item ytest\_list = y\_test.values.tolist()
        \end{itemize}
\end{itemize}

\subsubsection{GPU}
After setting everything up and starting with my work, I realized that unfortunately the machine upgrade was not enough for my work.
In fact, in the virtual machine was present only the CPU but not a GPU, extending considerably the time it took the CPU to train the model.
Fortunately, BFH has set up an online environment specifically for machine learning tasks, this env allows students to use jupyter notebooks supported by very powerful GPUs.

\subsubsection{Plotly}
\gls{MLMP} uses Jupyter-Lab and not Jupyter-Notebook, this difference is substantial regarding \gls{Plotly}. In fact, by installing and importing \gls{Plotly} as I would on a normal Jupyter-Notebook file I would have no problems.
The problem I had was that I was unable to \gls{plot} anything despite the imports.
I solved it thanks to these command lines in the \gls{MLMP} terminal:
\begin{enumerate}
    \item jupyter labextension list
    \item jupyter labextension install jupyterlab-plotly@4.8.2
    \item jupyter lab build
\end{enumerate}

In addition, I had to activate the Jupyter-Lab extensions and install:
\begin{enumerate}
    \item jupyterlab-chart-editor,
    \item make a rebuild.
\end{enumerate}

Once these steps are done \gls{Plotly} is set up correctly and running.

\subsection{Finding the appropriate model}
This task took me a long time, as for the sentiment analysis problem the field is really huge.
After several hours of research, I realized that \gls{BERT} is the state of the art, as far as this field is concerned.
The problem however is that even \gls{BERT} being a new technology is still not perfect and there is only one way to implement it.
Another obstacle is that \gls{BERT} is a macro-set of other models, so I spent more time looking for the \gls{BERT} model that was best suited.
However, when I found the right model, I had the opposite problem, that is, I had no room for improvement, as it was already perfect. 
I solved it by taking \gls{BERT} as a model to compare and creating something of my own even if not perfect.

\subsection{NewsApi}
NewsApi's dev(free) mode has limitations such as:
\begin{itemize}
    \item the content of articles is truncated to 200 characters,
    \item a limited number of requests per day,
\end{itemize}
So, these two limitations have blocked the work, especially the first one.
I wrote an email requesting a student-key just until the project is finished, but unfortunately no response.
I solved that I do the same calculation on polarity but not on all the text but on its description, which is a bit the summary of the article.
For the issue of the number of requests I solved with a script that I launch once a day.

The API adds at the end of the title "- name of the source", thus throwing off all the results of the model, I solved it by cleaning the title as described in \autoref{chap:model test newspaper}.