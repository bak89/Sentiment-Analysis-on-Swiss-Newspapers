@article{samuels_news_2020,
	title = {News Sentiment Analysis},
	url = {http://arxiv.org/abs/2007.02238},
	abstract = {Modern technological era has reshaped traditional lifestyle in several domains. The medium of publishing news and events has become faster with the advancement of Information Technology. {IT} has also been flooded with immense amounts of data, which is being published every minute of every day, by millions of users, in the shape of comments, blogs, news sharing through blogs, social media micro-blogging websites and many more. Manual traversal of such huge data is a challenging job, thus, sophisticated methods are acquired to perform this task automatically and efficiently. News reports events that comprise of emotions - good, bad, neutral. Sentiment analysis is utilized to investigate human emotions present in textual information. This paper presents a lexicon-based approach for sentiment analysis of news articles. The experiments have been performed on {BBC} news data set, which expresses the applicability and validation of the adopted approach.},
	journaltitle = {{arXiv}:2007.02238 [cs]},
	author = {Samuels, Antony and Mcgonical, John},
	urldate = {2021-04-10},
	date = {2020-07-05},
	eprinttype = {arxiv},
	eprint = {2007.02238},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\caima\\Zotero\\storage\\B6SMVDZ7\\Samuels e Mcgonical - 2020 - News Sentiment Analysis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\caima\\Zotero\\storage\\XHIQLW4E\\2007.html:text/html},
}

@online{noauthor_project_nodate,
	title = {Project Jupyter},
	url = {https://www.jupyter.org},
	abstract = {The Jupyter Notebook is a web-based interactive computing platform. The notebook combines live code, equations, narrative text, visualizations, interactive dashboards and other media.},
	urldate = {2021-04-10},
	file = {Snapshot:C\:\\Users\\caima\\Zotero\\storage\\Y6GTY73X\\jupyter.org.html:text/html},
}

@online{noauthor_kaggle_nodate,
	title = {Kaggle: Your Home for Data Science},
	url = {https://www.kaggle.com/},
	urldate = {2021-04-10},
	file = {Kaggle\: Your Home for Data Science:C\:\\Users\\caima\\Zotero\\storage\\AAJ7TV9B\\www.kaggle.com.html:text/html},
}

@online{zeeshan-ul-hassan_what_2018,
	title = {What is Kaggle, Why I Participate, What is the Impact? {\textbar} Data Science and Machine Learning},
	url = {https://www.kaggle.com/getting-started/44916},
	shorttitle = {What is Kaggle, Why I Participate, What is the Impact?},
	abstract = {What is Kaggle, Why I Participate, What is the Impact?.},
	titleaddon = {Getting Started},
	author = {Zeeshan-ul-hassan, Usmani},
	urldate = {2021-04-10},
	date = {2018},
	langid = {english},
	annotation = {Kaggle is an {AirBnB} for Data Scientists – this is where they spend their nights and weekends.},
}

@online{anaconda_inc_anaconda_nodate,
	title = {Anaconda {\textbar} The World's Most Popular Data Science Platform},
	url = {https://www.anaconda.com/},
	abstract = {Anaconda is the birthplace of Python data science. We are a movement of data scientists, data-driven enterprises, and open source communities.},
	titleaddon = {Anaconda},
	author = {Anaconda Inc},
	urldate = {2021-04-10},
	langid = {english},
	file = {Snapshot:C\:\\Users\\caima\\Zotero\\storage\\Z28YN5HQ\\www.anaconda.com.html:text/html},
}

@online{jetbrains_sro_pycharm_nodate,
	title = {{PyCharm}: the Python {IDE} for Professional Developers by {JetBrains}},
	url = {https://www.jetbrains.com/pycharm/},
	shorttitle = {{PyCharm}},
	abstract = {The Python \& Django {IDE} with intelligent code completion, on-the-fly error checking, quick-fixes, and much more...},
	titleaddon = {{JetBrains}},
	author = {{JetBrains} s.r.o.},
	urldate = {2021-04-10},
	langid = {english},
	file = {Snapshot:C\:\\Users\\caima\\Zotero\\storage\\ABJTIFU3\\pycharm.html:text/html},
}

@online{github_inc_github_nodate,
	title = {{GitHub}: Where the world builds software},
	url = {https://github.com/},
	shorttitle = {{GitHub}},
	abstract = {{GitHub} is where over 56 million developers shape the future of software, together. Contribute to the open source community, manage your Git repositories, review code like a pro, track bugs and feat...},
	titleaddon = {{GitHub}},
	author = {{GitHub} Inc},
	urldate = {2021-04-10},
	langid = {english},
	file = {Snapshot:C\:\\Users\\caima\\Zotero\\storage\\WL6BNNTL\\github.com.html:text/html},
}

@online{noauthor_overleaf_nodate,
	title = {Overleaf, Online {LaTeX} Editor},
	url = {https://www.overleaf.com},
	abstract = {An online {LaTeX} editor that's easy to use. No installation, real-time collaboration, version control, hundreds of {LaTeX} templates, and more.},
	urldate = {2021-04-10},
	langid = {english},
	file = {Snapshot:C\:\\Users\\caima\\Zotero\\storage\\WFV9LSLR\\www.overleaf.com.html:text/html},
}

@online{berner_fachhochschule_mlmp_nodate,
	title = {{MLMP}},
	url = {https://mlmp.ti.bfh.ch/vpn},
	author = {Berner Fachhochschule},
	urldate = {2021-04-10},
	file = {MLMP:C\:\\Users\\caima\\Zotero\\storage\\UYF9GFUA\\vpn.html:text/html},
}

@online{noauthor_scikit-learn_nodate,
	title = {scikit-learn: machine learning in Python — scikit-learn 0.24.1 documentation},
	url = {https://scikit-learn.org/stable/},
	urldate = {2021-04-10},
	file = {scikit-learn\: machine learning in Python — scikit-learn 0.24.1 documentation:C\:\\Users\\caima\\Zotero\\storage\\4486RCP3\\stable.html:text/html},
}

@online{noauthor_pandas_nodate,
	title = {pandas - Python Data Analysis Library},
	url = {https://pandas.pydata.org/},
	urldate = {2021-04-10},
	file = {pandas - Python Data Analysis Library:C\:\\Users\\caima\\Zotero\\storage\\3EXQI8JH\\pandas.pydata.org.html:text/html},
}

@online{noauthor_tensorflow_nodate,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/},
	abstract = {An end-to-end open source machine learning platform for everyone. Discover {TensorFlow}'s flexible ecosystem of tools, libraries and community resources.},
	titleaddon = {{TensorFlow}},
	urldate = {2021-04-10},
	langid = {english},
	file = {Snapshot:C\:\\Users\\caima\\Zotero\\storage\\Q4XVEPAA\\www.tensorflow.org.html:text/html},
}

@online{team_keras_nodate,
	title = {Keras documentation: Introduction to Keras for Engineers},
	url = {https://keras.io/getting_started/intro_to_keras_for_engineers/},
	shorttitle = {Keras documentation},
	abstract = {Keras documentation},
	author = {Team, Keras},
	urldate = {2021-04-10},
	langid = {english},
	file = {Snapshot:C\:\\Users\\caima\\Zotero\\storage\\X4P8JNXH\\intro_to_keras_for_engineers.html:text/html},
}

@online{noauthor_keras_nodate,
	title = {Keras: the Python deep learning {API}},
	url = {https://keras.io/},
	urldate = {2021-04-10},
	file = {Keras\: the Python deep learning API:C\:\\Users\\caima\\Zotero\\storage\\9XIUQNH8\\keras.io.html:text/html},
}

@article{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	journaltitle = {{arXiv}:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2021-04-10},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\caima\\Zotero\\storage\\DGKM9I7R\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\caima\\Zotero\\storage\\N65JWV4E\\1810.html:text/html},
}

@article{shazeer_talking-heads_2020,
	title = {Talking-Heads Attention},
	url = {http://arxiv.org/abs/2003.02436},
	abstract = {We introduce "talking-heads attention" - a variation on multi-head attention which includes linearprojections across the attention-heads dimension, immediately before and after the softmax operation.While inserting only a small number of additional parameters and a moderate amount of additionalcomputation, talking-heads attention leads to better perplexities on masked language modeling tasks, aswell as better quality when transfer-learning to language comprehension and question answering tasks.},
	journaltitle = {{arXiv}:2003.02436 [cs, eess, stat]},
	author = {Shazeer, Noam and Lan, Zhenzhong and Cheng, Youlong and Ding, Nan and Hou, Le},
	urldate = {2021-04-10},
	date = {2020-03-05},
	eprinttype = {arxiv},
	eprint = {2003.02436},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\caima\\Zotero\\storage\\FN4NCF4J\\Shazeer et al. - 2020 - Talking-Heads Attention.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\caima\\Zotero\\storage\\K6WN6ZJV\\2003.html:text/html},
}

@article{shazeer_glu_2020,
	title = {{GLU} Variants Improve Transformer},
	url = {http://arxiv.org/abs/2002.05202},
	abstract = {Gated Linear Units ({arXiv}:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on {GLU} are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer ({arXiv}:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used {ReLU} or {GELU} activations.},
	journaltitle = {{arXiv}:2002.05202 [cs, stat]},
	author = {Shazeer, Noam},
	urldate = {2021-04-10},
	date = {2020-02-12},
	eprinttype = {arxiv},
	eprint = {2002.05202},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\caima\\Zotero\\storage\\GAKMEEHS\\Shazeer - 2020 - GLU Variants Improve Transformer.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\caima\\Zotero\\storage\\NEIGRLN4\\2002.html:text/html},
}

@article{lan_albert_2020,
	title = {{ALBERT}: A Lite {BERT} for Self-supervised Learning of Language Representations},
	url = {http://arxiv.org/abs/1909.11942},
	shorttitle = {{ALBERT}},
	abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to {GPU}/{TPU} memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of {BERT}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original {BERT}. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the {GLUE}, {RACE}, and {\textbackslash}squad benchmarks while having fewer parameters compared to {BERT}-large. The code and the pretrained models are available at https://github.com/google-research/{ALBERT}.},
	journaltitle = {{arXiv}:1909.11942 [cs]},
	author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	urldate = {2021-04-10},
	date = {2020-02-08},
	eprinttype = {arxiv},
	eprint = {1909.11942},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\caima\\Zotero\\storage\\CKTDWJU3\\Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\caima\\Zotero\\storage\\W9CUI6XS\\1909.html:text/html},
}

@article{smit_chexbert_2020,
	title = {{CheXbert}: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using {BERT}},
	url = {http://arxiv.org/abs/2004.09167},
	shorttitle = {{CheXbert}},
	abstract = {The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts. In this work, we introduce a {BERT}-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations. We demonstrate superior performance of a biomedically pretrained {BERT} model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation. We find that our final model, {CheXbert}, is able to outperform the previous best rules-based labeler with statistical significance, setting a new {SOTA} for report labeling on one of the largest datasets of chest x-rays.},
	journaltitle = {{arXiv}:2004.09167 [cs]},
	author = {Smit, Akshay and Jain, Saahil and Rajpurkar, Pranav and Pareek, Anuj and Ng, Andrew Y. and Lungren, Matthew P.},
	urldate = {2021-04-10},
	date = {2020-10-18},
	eprinttype = {arxiv},
	eprint = {2004.09167},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	annotation = {Comment: Accepted to {EMNLP} 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\caima\\Zotero\\storage\\AF53JE9U\\Smit et al. - 2020 - CheXbert Combining Automatic Labelers and Expert .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\caima\\Zotero\\storage\\L5TB3542\\2004.html:text/html},
}

@article{clark_electra_2020,
	title = {{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators},
	url = {http://arxiv.org/abs/2003.10555},
	shorttitle = {{ELECTRA}},
	abstract = {Masked language modeling ({MLM}) pre-training methods such as {BERT} corrupt the input by replacing some tokens with [{MASK}] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream {NLP} tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than {MLM} because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by {BERT} given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one {GPU} for 4 days that outperforms {GPT} (trained using 30x more compute) on the {GLUE} natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to {RoBERTa} and {XLNet} while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
	journaltitle = {{arXiv}:2003.10555 [cs]},
	author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
	urldate = {2021-04-10},
	date = {2020-03-23},
	eprinttype = {arxiv},
	eprint = {2003.10555},
	keywords = {Computer Science - Computation and Language},
	annotation = {Comment: {ICLR} 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\caima\\Zotero\\storage\\QIMFVWM8\\Clark et al. - 2020 - ELECTRA Pre-training Text Encoders as Discriminat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\caima\\Zotero\\storage\\4KRZFAS2\\2003.html:text/html},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2021-04-10},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annotation = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\caima\\Zotero\\storage\\NS4CN3HX\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\caima\\Zotero\\storage\\E7E6NF88\\1706.html:text/html},
}

@software{maiya_amaiyaktrain_2021,
	title = {amaiya/ktrain},
	rights = {Apache-2.0 License, Apache-2.0 License},
	url = {https://github.com/amaiya/ktrain},
	abstract = {ktrain is a Python library that makes deep learning and {AI} more accessible and easier to apply},
	author = {Maiya, Arun S.},
	urldate = {2021-04-10},
	date = {2021-04-09},
	note = {original-date: 2019-02-06T17:01:39Z},
	keywords = {computer-vision, deep-learning, graph-neural-networks, keras, machine-learning, nlp, python, tabular-data, tensorflow},
}


@online{noauthor_pytorch_nodate,
	title = {{PyTorch}},
	url = {https://www.pytorch.org},
	abstract = {An open source deep learning platform that provides a seamless path from research prototyping to production deployment.},
	urldate = {2021-04-10},
	langid = {english},
	file = {Snapshot:C\:\\Users\\caima\\Zotero\\storage\\M8PBY3N8\\pytorch.org.html:text/html},
}

@software{noauthor_huggingfacetransformers_2021,
	title = {huggingface/transformers},
	rights = {Apache-2.0 License         ,                 Apache-2.0 License},
	url = {https://github.com/huggingface/transformers},
	abstract = {��Transformers: State-of-the-art Natural Language Processing for Pytorch and {TensorFlow} 2.0.},
	publisher = {Hugging Face},
	urldate = {2021-04-10},
	date = {2021-04-10},
	note = {original-date: 2018-10-29T13:56:00Z},
	keywords = {bert, gpt, hacktoberfest-accepted, language-model, language-models, model-hub, natural-language-generation, natural-language-processing, natural-language-understanding, nlp, nlp-library, pretrained-models, pytorch, pytorch-transformers, tensorflow, transformer, transformer-xl, xlm, xlnet},
}

@online{noauthor_sentiment_nodate,
	title = {Sentiment Analysis},
	url = {https://ai.stanford.edu/~amaas/data/sentiment/},
	urldate = {2021-04-10},
}

@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}


@online{noauthor_tfkerasoptimizersadam_nodate,
	title = {tf.keras.optimizers.Adam {\textbar} {TensorFlow} Core v2.4.1},
	url = {https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam},
	abstract = {Optimizer that implements the Adam algorithm.},
	titleaddon = {{TensorFlow}},
	urldate = {2021-04-11},
	langid = {english},
}

@online{noauthor_transformersmodelsbertmodeling_tf_bert_nodate,
	title = {transformers.models.bert.modeling\_tf\_bert — transformers 4.5.0.dev0 documentation},
	url = {https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_tf_bert.html#TFBertForSequenceClassification},
	urldate = {2021-04-12},
	file = {transformers.models.bert.modeling_tf_bert — transformers 4.5.0.dev0 documentation:C\:\\Users\\caima\\Zotero\\storage\\BGCUD4LH\\modeling_tf_bert.html:text/html},
}

@online{noauthor_transformersdataprocessorsutils_nodate,
	title = {transformers.data.processors.utils — transformers 4.5.0.dev0 documentation},
	url = {https://huggingface.co/transformers/_modules/transformers/data/processors/utils.html#InputExample},
	urldate = {2021-04-12},
}

@online{noauthor_tensorflow_nodate_optimizer,
	title = {{TensorFlow} - Optimizers - Tutorialspoint},
	url = {https://www.tutorialspoint.com/tensorflow/tensorflow_optimizers.htm},
	urldate = {2021-04-12},
}

@online{patnaik_loss_2018,
	title = {Loss Function in {TensorFlow}},
	url = {https://medium.datadriveninvestor.com/loss-function-in-tensorflow-b7eb1215ef78},
	abstract = {In machine learning you develop a model, which is a hypothesis, to predict a value given a set of input values. The model has a set of…},
	titleaddon = {Medium},
	author = {Patnaik, Amaresh},
	urldate = {2021-04-12},
	date = {2018-12-30},
	langid = {english},
}

@online{noauthor_tfkeraslossessparse_categorical_crossentropy_nodate,
	title = {tf.keras.losses.sparse\_categorical\_crossentropy},
	url = {https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy},
	abstract = {Computes the sparse categorical crossentropy loss.},
	titleaddon = {{TensorFlow}},
	urldate = {2021-04-12},
	langid = {english},
}

@online{noauthor_tfkerasmetricssparsecategoricalaccuracy_nodate,
	title = {tf.keras.metrics.{SparseCategoricalAccuracy} {\textbar} {TensorFlow} Core v2.4.1},
	url = {https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy},
	abstract = {Calculates how often predictions matches integer labels.},
	titleaddon = {{TensorFlow}},
	urldate = {2021-04-12},
	langid = {english},
}

@online{noauthor_amaiyaktrain_nodate,
	title = {amaiya/ktrain text},
	url = {https://github.com/amaiya/ktrain/blob/master/ktrain/text/data.py},
	abstract = {ktrain is a Python library that makes deep learning and {AI} more accessible and easier to apply - amaiya/ktrain},
	titleaddon = {{GitHub}},
	urldate = {2021-04-12},
	langid = {english},
}

@article{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	url = {http://arxiv.org/abs/1910.01108},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing ({NLP}), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called {DistilBERT}, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a {BERT} model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	journaltitle = {{arXiv}:1910.01108 [cs]},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	urldate = {2021-04-12},
	date = {2020-02-29},
	eprinttype = {arxiv},
	eprint = {1910.01108},
	keywords = {Computer Science - Computation and Language},
	annotation = {Comment: February 2020 - Revision: fix bug in evaluation metrics, updated metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - {NeurIPS} 2019},
}

@online{noauthor_distilbert_nodate,
	title = {{DistilBERT}},
	url = {https://huggingface.co/transformers/model_doc/distilbert.html},
	abstract = {Overview: The {DistilBERT} model was proposed in the blog post Smaller, faster, cheaper, lighter: Introducing {DistilBERT}, a distilled version of {BERT}, and the ...},
	urldate = {2021-04-12},
	langid = {english},
}

@article{smith_disciplined_2018,
	title = {A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay},
	url = {http://arxiv.org/abs/1803.09820},
	shorttitle = {A disciplined approach to neural network hyper-parameters},
	abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.},
	journaltitle = {{arXiv}:1803.09820 [cs, stat]},
	author = {Smith, Leslie N.},
	urldate = {2021-04-12},
	date = {2018-04-24},
	eprinttype = {arxiv},
	eprint = {1803.09820},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annotation = {Comment: Files to help replicate the results reported here are available on Github},
}

@article{smith_cyclical_2017,
	title = {Cyclical Learning Rates for Training Neural Networks},
	url = {http://arxiv.org/abs/1506.01186},
	abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the {CIFAR}-10 and {CIFAR}-100 datasets with {ResNets}, Stochastic Depth networks, and {DenseNets}, and the {ImageNet} dataset with the {AlexNet} and {GoogLeNet} architectures. These are practical tools for everyone who trains neural networks.},
	journaltitle = {{arXiv}:1506.01186 [cs]},
	author = {Smith, Leslie N.},
	urldate = {2021-04-12},
	date = {2017-04-04},
	eprinttype = {arxiv},
	eprint = {1506.01186},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annotation = {Comment: Presented at {WACV} 2017; see https://github.com/bckenstler/{CLR} for instructions to implement {CLR} in Keras},
}

@online{noauthor_amaiyaktrainautofit_nodate,
	title = {amaiya/ktrain/autofit},
	url = {https://github.com/amaiya/ktrain/blob/master/ktrain/core.py},
	abstract = {ktrain is a Python library that makes deep learning and {AI} more accessible and easier to apply - amaiya/ktrain},
	titleaddon = {{GitHub}},
	urldate = {2021-04-12},
	langid = {english},
}

@online{jupyter_1204,
	title = {Jupyter Notebook Viewer},
	url = {https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-02-tuning-learning-rates.ipynb},
	urldate = {2021-04-12},
}

@book{ravichandiran_getting_2021,
	title = {Getting Started with Google {BERT}: Build and train state-of-the-art natural language processing models using {BERT}},
	shorttitle = {Getting Started with Google {BERT}},
	author = {Ravichandiran, Sudharsan},
	date = {2021},
	note = {{ISBN}: 9781838826239
{OCLC}: 1241685882},
}

@software{guhr_oliverguhrgerman-sentiment_2021,
	title = {oliverguhr/german-sentiment},
	rights = {{MIT} License         ,                 {MIT} License},
	url = {https://github.com/oliverguhr/german-sentiment},
	abstract = {A data set and model for german sentiment classification.},
	author = {Guhr, Oliver},
	urldate = {2021-05-10},
	date = {2021-04-09},
	note = {original-date: 2019-12-02T13:32:51Z},
	keywords = {bert-model, deep-learning, fasttext, german-language, machine-learning, sentiment-analysis, sentiment-classification, transformer},
}

@online{noauthor_hugging_nodate,
	title = {Hugging Face – The {AI} community building the future.},
	url = {https://huggingface.co/models},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2021-05-10},
}

@online{noauthor_dbmdzbert-base-german-uncased_nodate,
	title = {dbmdz/bert-base-german-uncased · Hugging Face},
	url = {https://huggingface.co/dbmdz/bert-base-german-uncased},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2021-05-10},
}

@online{noauthor_open_nodate,
	title = {Open Source at the Bayerische Staatsbibliothek},
	url = {https://github.com/dbmdz},
	abstract = {...from the {MDZ} Digital Library team at the Bavarian State Library - Open Source at the Bayerische Staatsbibliothek},
	titleaddon = {{GitHub}},
	urldate = {2021-05-10},
	langid = {english},
}

@online{noauthor_munich_nodate,
	title = {Munich Digitization Center ({MDZ}) - Homepage},
	url = {https://www.digitale-sammlungen.de/en/},
	urldate = {2021-05-10},
}

@online{noauthor_news_nodate,
	title = {News {API} – Search News and Blog Articles on the Web},
	url = {https://newsapi.org},
	abstract = {Get {JSON} search results for global news articles in real-time with our free News {API}.},
	titleaddon = {News {API}},
	urldate = {2021-05-10},
	langid = {english},
}

@online{noauthor_gnews_nodate,
	title = {{GNews} {API} - An alternative to the Google News {API}},
	url = {https://gnews.io/},
	urldate = {2021-05-10},
}

@article{ribeiro_why_2016,
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	url = {http://arxiv.org/abs/1602.04938},
	shorttitle = {"Why Should I Trust You?},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose {LIME}, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	journaltitle = {{arXiv}:1602.04938 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2021-05-10},
	date = {2016-08-09},
	eprinttype = {arxiv},
	eprint = {1602.04938},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@software{ribeiro_marcotcrlime_2021,
	title = {marcotcr/lime},
	rights = {{BSD}-2-Clause License         ,                 {BSD}-2-Clause License},
	url = {https://github.com/marcotcr/lime},
	abstract = {Lime: Explaining the predictions of any machine learning classifier},
	author = {Ribeiro, Marco Tulio Correia},
	urldate = {2021-05-10},
	date = {2021-05-10},
	note = {original-date: 2016-03-15T22:18:10Z},
}

@video{giphy_blown_nodate,
	title = {Blown Away {GIF} - Find \& Share on {GIPHY}},
	url = {https://media0.giphy.com/media/Y1YNVL8SUm5v5WeNl5/giphy.gif?cid=790b7611b589b8521fc262e478a94f529e46a11e6de4dd03&rid=giphy.gif&ct=g},
	abstract = {Discover \& share this Blown Away {GIF} with everyone you know. {GIPHY} is how you search, share, discover, and create {GIFs}.},
	author = {{GIPHY}},
	urldate = {2021-05-10},
}





@online{noauthor_plotly_nodate,
	title = {Plotly: The front end for {ML} and data science models},
	url = {/},
	shorttitle = {Plotly},
	abstract = {Plotly creates \& stewards the leading data viz \& {UI} tools for {ML}, data science, engineering, and the sciences. Language support for Python, R, Julia, and {JavaScript}.},
	urldate = {2021-05-20},
	langid = {english},
}

@online{noauthor_plotly_nodate-1,
	title = {Plotly Python Graphing Library},
	url = {https://plotly.com/python/},
	abstract = {Plotly's Python graphing library makes interactive, publication-quality graphs. Examples of how to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts.},
	urldate = {2021-05-20},
}

@online{noauthor_dash_nodate,
	title = {Dash Documentation \& User Guide {\textbar} Plotly},
	url = {https://dash.plotly.com/},
	urldate = {2021-05-20},
}

@online{gugger_1cycle_nodate,
	title = {The 1cycle policy},
	url = {/the-1cycle-policy.html},
	abstract = {Properly setting the hyper-parameters of a neural network can be challenging, fortunately, there are some recipe that can help.},
	titleaddon = {Another data science student's blog},
	author = {Gugger, Sylvain},
	urldate = {2021-05-20},
	langid = {english},
	note = {Section: Experiments},
}

@online{noauthor_what_2019,
	title = {What is Tokenization {\textbar} Methods to Perform Tokenization},
	url = {https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/},
	abstract = {Looking to get started with {NLP}? We take a step by step look starting with what is tokenization and methods to perform tokenization for {NLP} tasks.},
	titleaddon = {Analytics Vidhya},
	urldate = {2021-05-23},
	date = {2019-07-18},
}

@online{brownlee_how_2020,
	title = {How to Calculate Precision, Recall, and F-Measure for Imbalanced Classification},
	url = {https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/},
	abstract = {Classification accuracy is the total number of correct predictions divided by the total number of predictions made for a dataset. […]},
	titleaddon = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	urldate = {2021-05-23},
	date = {2020-01-02},
	langid = {american},
}

@online{noauthor_textexplainer_nodate,
	title = {{TextExplainer}: debugging black-box text classifiers — {ELI}5 0.11.0 documentation},
	url = {https://eli5.readthedocs.io/en/latest/tutorials/black-box-text-classifiers.html},
	urldate = {2021-05-23},
}


@misc{tutorial,
	title = {Sentiment {Analysis} with {TensorFlow} 2 and {Keras} using {Python}},
	abstract = {Learn how to detect sentiment in hotel reviews using embeddings. Build and train a Deep Neural Network for text classification.},
	howpublished = {Available on: \url{https://curiousily.com/posts/sentiment-analysis-with-tensorflow-2-and-keras-using-python/}},
    note = {Accessed: 12.10.2020}
}

@misc{tutorial_keras,
	title = {Keras {Tutorial}: {The} {Ultimate} {Beginner}'s {Guide} to {Deep} {Learning} in {Python}},
	shorttitle = {Keras {Tutorial}},
	url = {https://elitedatascience.com/keras-tutorial-deep-learning-in-python},
	abstract = {Step-by-step Keras tutorial for how to build a convolutional neural network in Python. We'll train a classifier for MNIST that boasts over 99\% accuracy.},
}

@misc{colab,
    author = {Alphabet Inc.},
	title = {Google {Colaboratory}},
	howpublished = {Available on: \url{https://colab.research.google.com/notebooks/intro.ipynb}},
	note = {Accessed: 12.10.2020}
}
 
 
@misc{tensorflow,
	title = {{TensorFlow}},
	abstract = {An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources.},
	howpublished = {Available on: \url{https://www.tensorflow.org/}},
	note = {Accessed: 12.10.2020}
}


@misc{khan_natural_2020,
	author = {Khan, Hasan Faraz},
	title = {Natural {Language} {Processing} and {Sentiment} {Analysis} using {Tensorflow}.},
	abstract = {Natural Language Processing or NLP is a field of Artificial Intelligence and Deep Learning that gives the machines the ability to read…},
	howpublished = {Available on: \url{https://levelup.gitconnected.com/natural-language-processing-and-sentiment-analysis-using-tensorflow-c2948f2623f}},
	note = {Accessed: 05.10.2020}
}

@misc{karikari_deep_2020,
	author = {Karikari, Paul},
	title = {Deep {Learning} {LSTM} for {Sentiment} {Analysis} in {Tensorflow} with {Keras} {API}.},
	abstract = {Learn how to use LSTM for text classification in Tensorflow using Keras API},
	howpublished = {Available on: \url{https://medium.com/datadriveninvestor/deep-learning-lstm-for-sentiment-analysis-in-tensorflow-with-keras-api-92e62cde7626}},
	note = {Accessed: 25.12.2020}
}

@misc{topDataset2020,
    author ={},
	title = {Top 10 {Established} {Datasets} for {Sentiment} {Analysis} in 2020},
	abstract = {Sentiment analysis is the technique used for understanding people’s emotions and feelings, with the help of machine learning.},
	howpublished = {Available on: \url{https://www.upgrad.com/blog/established-datasets-for-sentiment-analysis/}},
	note = {Accessed: 15.11.2020}
}

@misc{515k_kaggle,
    author = {Jiashen Liu},
	title = {{515K} {Hotel} {Reviews} {Data} in {Europe}},
	abstract = {Can you make your trip more cozy by using data science?},
	howpublished = {Available on: \url{https://kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe}},
	note = {Accessed: 25.10.2020}
}

@misc{git,
	author = {Valkov, Venelin},
	title = {curiousily/{Deep}-{Learning}-{For}-{Hackers}},
	copyright = {MIT License,MIT License},
	abstract = {Machine Learning tutorials with TensorFlow 2 and Keras in Python (Jupyter notebooks included)},
	howpublished = {Available on: \url{https://github.com/curiousily/Deep-Learning-For-Hackers}},
    note = {Accessed: 25.11.2020}
}


@misc{keras,
    author = {Keras},
	title = {Model training APIs},
	abstract = {Model training APIs},
	howpublished = {Available on: \url{https://keras.io/api/models/model_training_apis/}},
	note = {Accessed: 14.01.2021}
}



@misc{python_sentiment_nodate,
	title = {Sentiment {Analysis}: {First} {Steps} {With} {Python}'s {NLTK} {Library} – {Real} {Python}},
	shorttitle = {Sentiment {Analysis}},
	url = {https://realpython.com/python-nltk-sentiment-analysis/},
	abstract = {In this tutorial, you'll learn how to work with Python's Natural Language Toolkit (NLTK) to process and analyze text. You'll also learn how to perform sentiment analysis with built-in as well as custom classifiers!},
	language = {en},
	urldate = {2021-03-14},
	author = {Python, Real},
}

@misc{pandey_sentiment_2020,
	title = {Sentiment {Analysis} using {Deep} {Learning} with {Tensorflow}},
	url = {https://medium.com/analytics-vidhya/sentiment-analysis-using-deep-learning-with-tensorflow-2bb176c40257},
	abstract = {Sentiment Analysis 
Sentiment analysis is the contextual study that aims to determine the opinions, feelings, outlooks, moods and emotions…},
	language = {en},
	urldate = {2021-03-14},
	journal = {Medium},
	author = {Pandey, Harshita},
	month = may,
	year = {2020},
}

@misc{python_practical_nodate,
	title = {Practical {Text} {Classification} {With} {Python} and {Keras} – {Real} {Python}},
	url = {https://realpython.com/python-keras-text-classification/},
	abstract = {Learn about Python text classification with Keras. Work your way from a bag-of-words model with logistic regression to more advanced methods leading to convolutional neural networks. See why word embeddings are useful and how you can use pretrained word embeddings. Use hyperparameter optimization to squeeze more performance out of your model.},
	language = {en},
	urldate = {2021-03-14},
	author = {Python, Real},
	file = {Snapshot:C\:\\Users\\caima\\Zotero\\storage\\QUNMZZDU\\python-keras-text-classification.html:text/html},
}

@misc{noauthor_classify_nodate,
	title = {Classify text with {BERT} {\textbar} {TensorFlow} {Core}},
	url = {https://www.tensorflow.org/tutorials/text/classify_text_with_bert},
	language = {en},
	urldate = {2021-03-14},
	journal = {TensorFlow},
}

@misc{girdhar_text_2020,
	title = {Text {Classification} with {BERT} using {Transformers} for long text inputs},
	url = {https://medium.com/analytics-vidhya/text-classification-with-bert-using-transformers-for-long-text-inputs-f54833994dfd},
	abstract = {Text classification has been one of the most popular topics in NLP and with the advancement of research in NLP over the last few years, we…},
	language = {en},
	urldate = {2021-03-14},
	journal = {Medium},
	author = {Girdhar, Dipansh},
	month = jun,
	year = {2020},
	file = {Snapshot:C\:\\Users\\caima\\Zotero\\storage\\CHXV6AQP\\text-classification-with-bert-using-transformers-for-long-text-inputs-f54833994dfd.html:text/html},
}

@misc{rajapakse_simple_2019,
	title = {Simple {Transformers} — {Multi}-{Class} {Text} {Classification} with {BERT}, {RoBERTa}, {XLNet}, {XLM}, and…},
	url = {https://medium.com/swlh/simple-transformers-multi-class-text-classification-with-bert-roberta-xlnet-xlm-and-8b585000ce3a},
	abstract = {Simple Transformers is the “it just works” Transformer library for real-world applications. Use Transformers with just 3 lines of code!},
	language = {en},
	urldate = {2021-03-14},
	journal = {Medium},
	author = {Rajapakse, Thilina},
	month = nov,
	year = {2019},
	file = {Snapshot:C\:\\Users\\caima\\Zotero\\storage\\RQGQNNAA\\simple-transformers-multi-class-text-classification-with-bert-roberta-xlnet-xlm-and-8b585000ce3.html:text/html},
}

@misc{antyukhov_fine-tuning_2020,
	title = {Fine-tuning {BERT} with {Keras} and tf.{Module}},
	url = {https://towardsdatascience.com/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2},
	abstract = {In this experiment we convert a pre-trained BERT model checkpoint into a trainable Keras layer to solve a text classification problem.},
	language = {en},
	urldate = {2021-03-14},
	journal = {Medium},
	author = {Antyukhov, Denis},
	month = mar,
	year = {2020},
	file = {Snapshot:C\:\\Users\\caima\\Zotero\\storage\\5HHZSPXS\\fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2.html:text/html},
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}