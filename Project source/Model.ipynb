{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Model Jupyter Notebook\n",
    "\n",
    "_Giorgio Bakhiet Derias_\n",
    "_I3a, Bachelorarbeit_\n",
    "\n",
    "The aim of this notebook is to show the process of creating a sentiment analysis model which reads text input and is able to attribute an emotion to it.\n",
    "To train the model, I use several different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "These are the required python libraries that are used in sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install from requirements\n",
    "In order to work I first need to install the libraries from which I will then import what I need.\n",
    "Since I moved my work from a Colab file to here, I created a text file called *requirementsModel*, in which I saved all the libraries I used.\n",
    "The usefulness of this file is when I move to a new environment, installing all packages at once by simply typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#%conda install --file requirementsModel.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether packages need updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Installing other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install gdown\n",
    "!pip install -q tf-models-official\n",
    "!pip install tensorflow-gpu\n",
    "!pip install transformers\n",
    "!pip install plotly-express\n",
    "!pip3 install ktrain\n",
    "!pip3 install git+https://github.com/amaiya/eli5@tfkeras_0_10_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create requirements file\n",
    "!pip3 freeze > requirementsModel.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Once installed the packages we need we can move on to importing the various libraries that will be used during the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Numpy and Pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from pylab import rcParams\n",
    "\n",
    "# Seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# KTrain\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You are using TensorFlow version\", tf.__version__)\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"You have a GPU enabled.\")\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "else:\n",
    "    print(\"Enable a GPU before running this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Datasets\n",
    "This section goes more in depth about the data set. Specifically what kind of data it contains and how it is structured.\n",
    "For my work, I have been working on several datasets, so that I can later make several tests with the model, and see which dataset trains my model best.\n",
    "\n",
    "At the end I used for German language:\n",
    "- **\"Filmstarts dataset\"** available at https://zenodo.org/record/3693810/files/sentiment-data-reviews-and-neutral.zip?download=1\n",
    "\n",
    "Filmstarts dataset is about movie reviews in German.\n",
    "\n",
    "## Filmstarts dataset\n",
    "\n",
    "First I import the dataset I downloaded earlier using the pandas function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data using pandas\n",
    "film_de = pd.read_csv(\"filmstarts.tsv\", sep = '\\t',encoding='utf8', error_bad_lines=False, warn_bad_lines=True, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes (Columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check the dataset\n",
    "film_de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and resample the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_drop = [0]\n",
    "film_de.drop(columns_to_drop, axis=\"columns\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "film_de = film_de[[2,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "film_de = film_de.rename(columns={2: 'Review', 1: 'Score'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "film_de.loc[film_de.Score <= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_de.at[11,\"Review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe inspection\n",
    "\n",
    "Now that I've cleaned up the dataframes with what I needed, I see how they are composed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "film_de.Score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(\n",
    "    x=\"Score\",\n",
    "    data=film_de,\n",
    "    order=film_de.Score.value_counts().index\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Review type\")\n",
    "plt.ylabel(\"Number of review\")\n",
    "plt.title(\"Review types displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Input and Response Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Input dataframe contains the features that are the input for the learning and decision making of the machine learning model.\n",
    "- The Response (a.k.a. Target) dataframe contains the correct expected values (a.k.a answers) that the system is suppposed to learn.\n",
    "\n",
    "As Input I take `\"Review\"`\n",
    "\n",
    "As Target I take `\"Score\"`\n",
    "\n",
    "Additionally I create a new column `\"Positive\"` that contains labels describing how good or bad the review score is. The evaluation is done by the following criteria:\n",
    "*   **\"0\"** up to score 1\n",
    "*   **\"1\"** up to score 5\n",
    "\n",
    "To achieve this, I write a function to have only positive and negative polarity using the `\"Score\"` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get review type by aforementioned method\n",
    "def get_review_type(review_score):\n",
    "    if review_score <= 0:\n",
    "        return 0\n",
    "    elif review_score >= 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "film_de[\"Positive\"] = film_de[\"Score\"].apply(\n",
    "  lambda x: get_review_type(x)\n",
    ")\n",
    "\n",
    "# Combine only the useful columns\n",
    "film_df_de = film_de[[\"Review\", \"Positive\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the dataframe after the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "film_df_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(\n",
    "    x=\"Positive\",\n",
    "    data=film_df_de,\n",
    "    order=film_df_de.Positive.value_counts().index\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Review type\")\n",
    "plt.ylabel(\"Number of review\")\n",
    "plt.title(\"Review types displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the two categories 1(\"good\") and 0(\"bad\").\n",
    "As can be seen from the chart the \"good\" category has many more values than the \"bad\" category, so we should limit the larger category to the value of the smaller one.\n",
    "By doing so, all categories will have an equal number of reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample reviews\n",
    "To prepare the data for sentiment analysis, it needs to be reshaped in the way that each review type has an equal number of reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get same number of reviews for each type\n",
    "bad_reviews = film_df_de[film_df_de.Positive == 0]\n",
    "good_reviews = film_df_de[film_df_de.Positive == 1]\n",
    "\n",
    "\n",
    "sample_len = len(bad_reviews)\n",
    "\n",
    "bad_df = bad_reviews\n",
    "good_df = good_reviews.sample(n=sample_len, random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "film_review_df = good_df.append(bad_df).reset_index(drop=True)\n",
    "film_review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Display number of each review type\n",
    "sns.countplot(\n",
    "  x='Positive',\n",
    "  data=film_review_df,\n",
    "  order=film_review_df.Positive.value_counts().index\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Review type\")\n",
    "plt.title(\"All review types (resampled)\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Filmstarts\n",
    "\n",
    "Check the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_review_df = film_review_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "film_review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "### Choosing Sequence Length\n",
    "\n",
    "BERT works with fixed-length sequences. We'll use a simple strategy to choose the max length. Let's store the token length of each review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ktrain along with a couple things from transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer_hugg = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
    "#model_hugg = AutoModel.from_pretrained(\"dbmdz/bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textToCheck = film_review_df.Review[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textToCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in textToCheck:\n",
    "    tokens = tokenizer_hugg.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 500]);\n",
    "plt.ylim([0, 1.05])\n",
    "plt.xlabel('Token count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare all what I need for the ktrain functions.\n",
    "I need to define the 2 categories and train/validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Positive', 'Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(film_review_df,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = (train.Review, train.Positive)\n",
    "(x_test, y_test) = (test.Review, test.Positive)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of training set: %s' % (len(train['Review'])))\n",
    "print('size of test set: %s' % (len(test['Review'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use my sets in ktrain I have to transform them into lists first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_list = x_train.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_list = y_train.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_list = x_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_list = y_test.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "### List problem \n",
    "\n",
    "A couple of reviews were empty, so I had an error and could not process it.\n",
    "\n",
    "AttributeError: 'float' object has no attribute 'split'\n",
    "\n",
    "I solved this by removing all nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test['Review'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_test, y_test) = (test.Review, test.Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_list = x_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_list = y_test.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------\n",
    "# Build a Model and Wrap in Learner\n",
    "Now I put all the pieces together for ktrain, first I define the pre-trained model and the transformer.\n",
    "Then I will do the preprocessing of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'dbmdz/bert-base-german-cased'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = text.Transformer(MODEL_NAME, maxlen=400, class_names=['0','1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = t.preprocess_train(xtrain_list,ytrain_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = t.preprocess_test(xtest_list, ytest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = t.get_classifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have a model I will wrap it in a ktrain learner, I will use this learner to find the best learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ktrain.get_learner(model, train_data=trn, val_data=tst, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find(show_plot=True, max_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose a learning rate from -5, now I can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.autofit(3e-5, reduce_on_plateau=3, checkpoint_folder='./checkpointNewModel26.05/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model has an accuracy of 93%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_de.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model_de, to_file='model.png', show_shapes=True, show_dtype=False,\n",
    "    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and Preprocessor instance after partially training\n",
    "#ktrain.get_predictor(model_de, preproc).save('./model_save/predictor_22.04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model using transformers API after partially training\n",
    "#learner_de.model.save('./model_save/my_model_de_22.04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model using transformers API after partially training\n",
    "#learner_de.model.save_pretrained('./model_save/my_model_smallde_22.04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.model)\n",
    "print(predictor.preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model using ktrain predictor, after train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.save('./modelsave/bertDe_predictor_93')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------\n",
    "# Reload Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload predictor\n",
    "predictor = ktrain.load_predictor('./modelsave/bertDe_predictor_93')\n",
    "predictor.predict('Heute ist ein schöner Tag.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "# Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict('Heute ist ein schlecte Tag.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict('Heute ist ein schöner Tag.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.view_top_losses(n=1, preproc=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtest_list[553])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict_proba(xtest_list[553])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.get_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txTest = \"Philip liebte den Pferdesport – genau wie seine Enkelin Louise. Die Tochter von Prinz Edward soll nun Philips Kutsche und die zwei Lieblingsponys erben.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txTest2 = \"Ein Brand in der südafrikanischen Metropole hat auch Flächen des berühmten Tafelbergs in Mitleidenschaft gezogen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txTest3 = \"Prinz Philip: Enkelin Louise bekommt seine geliebten Ponys - 20 Minuten\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txTest4 = \"Prinz Philip: Enkelin Louise bekommt seine geliebten Ponys.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(txTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.explain(txTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.explain(txTest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.explain(txTest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.explain(txTest4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.explain(\"Eskalationsrisiko: Russland stationiert \\\"mehr als 150’000 Soldaten\\\" an der Grenze zur Ukraine. Der EU-Aussenbeauftragte Josep Borrell zeigt sich besorgt über die Lage an der ukrainischen Grenze. Russland sei mit mindestens 150’000 Soldaten aufmarschiert, es sei der grösste Aufmarsch, den es je in Russland gegeben habe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.explain(\"Der EU-Aussenbeauftragte Josep Borrell zeigt sich besorgt über die Lage an der ukrainischen Grenze. Russland sei mit mindestens 150’000 Soldaten aufmarschiert, es sei der grösste Aufmarsch, den es je in Russland gegeben habe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Export Model to tensorflow lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export TensorFlow Lite model\n",
    "tflite_model_path = './tensorFlowLite/model.tflite'\n",
    "tflite_model_path = predictor.export_model_to_tflite(tflite_model_path)\n",
    "\n",
    "# load interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# set maxlen, class_names, and tokenizer (use settings employed when training the model - see above)\n",
    "maxlen = 400                                                                       # from above\n",
    "class_names = ['0', '1'] # from above\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
    "\n",
    "# preprocess and predict outside of ktrain\n",
    "doc = 'Heute ist ein schöner Tag.'\n",
    "inputs = tokenizer(doc, max_length=maxlen, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "interpreter.set_tensor(input_details[0]['index'], inputs['attention_mask'])\n",
    "interpreter.set_tensor(input_details[1]['index'], inputs['input_ids'])\n",
    "interpreter.invoke()\n",
    "output_tflite = interpreter.get_tensor(output_details[0]['index'])\n",
    "print()\n",
    "print('text input: %s' % (doc))\n",
    "print()\n",
    "print('predicted logits: %s' % (output_tflite))\n",
    "print()\n",
    "print(\"predicted class: %s\" % ( class_names[np.argmax(output_tflite[0])]) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
